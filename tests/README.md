# SmartAudit-Lite: AI-Based Country-Level Risk Assessment from Media Content

This project is the codebase for my MSc Computer Science thesis at Middlesex University, titled:

> "SmartAudit-Lite: An AI-Based System for Real-Time Country-Level Loan and Business Risk Assessment via Media Content Analysis"

It uses a modular NLP pipeline to extract sentiment and named entities from real-world news data. The project structure follows a clean research code layout, with Jupyter notebooks, Python modules, and versioned data folders.

---

## ðŸ“ Repository Structure

RiskIntel/
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚ â”œâ”€â”€ 02_preprocessing.ipynb
â”‚ â””â”€â”€ 03_nlp_pipeline.ipynb
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_ingestion/ingest_news.py
â”‚ â”œâ”€â”€ preprocessing/clean_text.py
â”‚ â””â”€â”€ nlp/
â”‚ â”œâ”€â”€ sentiment_analysis.py
â”‚ â””â”€â”€ transformers_model.py (planned)
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/news_sample.json
â”‚ â”œâ”€â”€ processed/cleaned_articles.csv
â”‚ â””â”€â”€ processed/analyzed_articles.csv
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_clean_text.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

##  Progress Log

###  Day 1 â€“ Environment & Data Ingestion

- Set up conda environment (`.venv`, `requirements.txt`)
- Created modular folder structure: `src/`, `notebooks/`, `data/`
- Wrote `ingest_news.py` to load raw `.json` news data into DataFrame
- Logged ingestion workflow in `01_data_ingestion.ipynb`
- Saved raw data to `data/raw/news_sample.json`

> ðŸ“‚ Output: `data/raw/news_sample.json`

---

### Day 2 â€“ Text Cleaning & Preprocessing

- Built reusable cleaning function in `clean_text.py`
  - Lowercasing, punctuation removal, stopword removal, lemmatization
- Applied `clean_text()` to all raw news text
- Validated results in `02_preprocessing.ipynb`
- Saved cleaned data to `data/processed/cleaned_articles.csv`

> ðŸ“‚ Output: `data/processed/cleaned_articles.csv`

---

###  Day 3 â€“ NLP Pipeline: Sentiment & Entity Extraction

- Performed sentiment scoring using `TextBlob`
- Performed Named Entity Recognition (NER) using `spaCy`
  - Used `en_core_web_sm` model
  - Extracted geopolitical entities (e.g. â€œKenyaâ€, â€œUKâ€)
- Appended `sentiment_score` and `entities` columns to DataFrame
- Saved final NLP output to `analyzed_articles.csv`
- Tracked results in `03_nlp_pipeline.ipynb`

| title                 | sentiment_score | entities        |
|----------------------|-----------------|-----------------|
| Kenya inflation news | 0.0681          | [(kenya, GPE)]  |
| UK economy outlook   | 0.1667          | [(uk, GPE)]     |
| Empty article        | 0.0000          | []              |

> ðŸ“‚ Output: `data/processed/analyzed_articles.csv`

---

## ðŸ“Œ Next Steps

###  Day 4: Risk Labeling & Weak Supervision
- Assign initial risk categories to news (e.g., high/med/low)
- Explore keyword-based rule engine or Snorkel-style weak supervision
- Output: `risk_labeled_articles.csv`

###  Day 5: Exploratory Data Visualizations
- Word clouds, frequency plots, named entity bar charts
- Sentiment distribution histograms
- Country-specific risk trends

---

##  Tech Stack

- Python 3.10
- Jupyter Notebooks
- `pandas`, `spacy`, `textblob`, `nltk`
- Modular scripts in `src/`

---

##  Author

Emma Adhiambo Omingo  
MSc Computer Science, Middlesex University  
GitHub: [@EmmaOmingo](https://github.com/EmmaOmingo)  
Project Code Name: `RiskIntel`  
Thesis Title: *SmartAudit-Lite: AI-Based Real-Time Country Risk Scoring*

---

##  Citation (If reusing)



> Omingo, E. (2025). *SmartAudit-Lite: An AI-Based System for Real-Time Country-Level Loan and Business Risk Assessment via Media Content Analysis*. MSc Thesis, Middlesex University.

